{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Transformer model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Sentence Transformers from LlamaIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Load the Embedding Model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "print(\"Sentence Transformer model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /tmp/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 6.86 GiB (4.53 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 40\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 40\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 5120\n",
      "print_info: n_embd_v_gqa     = 5120\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 13824\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 13B\n",
      "print_info: model params     = 13.02 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: layer  37 assigned to device CPU\n",
      "load_tensors: layer  38 assigned to device CPU\n",
      "load_tensors: layer  39 assigned to device CPU\n",
      "load_tensors: layer  40 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_0) (and 82 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  6806.25 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  7023.90 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.33.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.33.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.36.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.36.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.36.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.36.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.36.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.36.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.36.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.37.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.37.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.38.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.38.ffn_down.weight with q4_0_8x8\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "llm = LlamaCPP(\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"/tmp/llama_index/models/llama-2-13b-chat.Q4_0.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=2900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Database Configuration\n",
    "db_name = \"vector_db\"\n",
    "host = \"localhost\"\n",
    "user = \"postgres\"  \n",
    "password = \"postgres\"  \n",
    "port = \"5432\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'vector_db' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize the Postgres Database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        host=host,\n",
    "        password=password,\n",
    "        port=port,\n",
    "        user=user,\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "    with conn.cursor() as c:\n",
    "        c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "        c.execute(f\"CREATE DATABASE {db_name}\")\n",
    "    print(f\"Database '{db_name}' created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Set up Vector Store in Postgres\n",
    "try:\n",
    "    vector_store = PGVectorStore.from_params(\n",
    "        database=db_name,\n",
    "        host=host,\n",
    "        password=password,\n",
    "        port=port,\n",
    "        user=user,\n",
    "        table_name=\"llama2_paper\",\n",
    "        embed_dim=384,  # Adjust based on your embedding model\n",
    "    )\n",
    "    print(\"Vector store initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing vector store: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build an Ingestion Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to data/llama2.pdf\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2307.09288.pdf\"\n",
    "output = \"data/llama2.pdf\"\n",
    "wget.download(url, output)\n",
    "print(f\"Downloaded to {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 77 document(s)!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "# Load PDF document\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split document into 107 text chunks!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Text splitting\n",
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",  # You can customize the separator if needed\n",
    ")\n",
    "text_chunks = []\n",
    "doc_idxs = []\n",
    "\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "\n",
    "print(f\"Split document into {len(text_chunks)} text chunks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed 107 nodes!\n"
     ]
    }
   ],
   "source": [
    "#Construct Nodes from Text Chunks\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)\n",
    "\n",
    "print(f\"Constructed {len(nodes)} nodes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated for all nodes!\n"
     ]
    }
   ],
   "source": [
    "# Generate Embeddings for Each Node\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding\n",
    "\n",
    "print(\"Embeddings generated for all nodes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.errors.UndefinedTable) relation \"public.data_llama2_paper\" does not exist\nLINE 1: INSERT INTO public.data_llama2_paper (text, metadata_, node_...\n                    ^\n\n[SQL: INSERT INTO public.data_llama2_paper (text, metadata_, node_id, embedding) SELECT p0::VARCHAR, p1::JSON, p2::VARCHAR, p3::VECTOR(384) FROM (VALUES (%(text__0)s, %(metadata___0)s::JSON, %(node_id__0)s, %(embedding__0)s, 0), (%(text__1)s, %(metadata___ ... 8585 characters truncated ... er) ORDER BY sen_counter RETURNING public.data_llama2_paper.id, public.data_llama2_paper.id AS id__1]\n[parameters: {'node_id__0': 'cbfc2660-d930-4a25-967e-15cf5a4f17a1', 'metadata___0': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"1\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"cbfc2660-d930-4a25-967e-15cf5a4f17a1\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__0': '[-0.04617450758814812,0.03491324186325073,-0.013553637079894543,-0.007586942054331303,0.017399096861481667,-0.00029551811167038977,-0.008765399456024 ... (7818 characters truncated) ... ,0.008555163629353046,-0.017789412289857864,0.004046207293868065,-0.029435161501169205,0.00034099709591828287,0.02046983316540718,0.0345381461083889]', 'text__0': 'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗\\nLouis Martin†\\nKevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay  ... (1626 characters truncated) ... scialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023', 'node_id__1': '3c480d46-9ad5-4048-a605-7768eff38c60', 'metadata___1': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"2\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"3c480d46-9ad5-4048-a605-7768eff38c60\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__1': '[-0.070559062063694,-0.02465185895562172,0.02275167964398861,-0.019947784021496773,0.020725281909108162,0.03974338248372078,-0.02047322317957878,0.00 ... (7838 characters truncated) ... -0.013566997833549976,-0.051573220640420914,0.027146410197019577,-0.04085184633731842,0.04744908958673477,0.013965633697807789,0.0020637046545743942]', 'text__1': 'Contents\\n1\\nIntroduction\\n3\\n2\\nPretraining\\n5\\n2.1\\nPretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ... (1093 characters truncated) ... ma 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n5\\nDiscussion\\n32\\n5.1\\nLearnings and Observations . . . . . . . .', 'node_id__2': '4416fde3-2678-44f4-acbf-2aa8d8802a8f', 'metadata___2': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"2\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"4416fde3-2678-44f4-acbf-2aa8d8802a8f\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__2': '[-0.05603757128119469,-0.01130712777376175,0.009145677089691162,-0.029648946598172188,0.053696755319833755,0.016262132674455643,-0.02239496260881424, ... (7826 characters truncated) ... 6,-0.012882216833531857,-0.04809080436825752,0.013987668789923191,-0.03842881694436073,0.01268172636628151,0.021383190527558327,0.021790459752082825]', 'text__2': '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n4.4\\nSafety Evaluation of Llama 2-Chat . . . . . . . . . . ... (1075 characters truncated) ...  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n75\\nA.7 Model Card\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .', 'node_id__3': 'f1124af1-7c62-4f44-8008-a74001c612af', 'metadata___3': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"2\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"f1124af1-7c62-4f44-8008-a74001c612af\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__3': '[-0.04469098523259163,-0.03675113990902901,0.031259194016456604,-0.00794018805027008,0.03586065024137497,0.0026993893552571535,-0.029911573976278305, ... (7826 characters truncated) ... -0.02732889913022518,-0.023450074717402458,0.018889112398028374,-0.03715568780899048,0.012513435445725918,-0.004459341522306204,0.006109000649303198]', 'text__3': '. . . . . . . . . . . . . . . . . . . . . . . .\\n72\\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n75\\nA.7 Model Card\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n2', 'node_id__4': '93f7bdb9-e0c7-4f47-a115-fcf64051ae26', 'metadata___4': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"3\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"93f7bdb9-e0c7-4f47-a115-fcf64051ae26\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__4': '[-0.030008263885974884,0.007953417487442493,0.015078099444508553,0.015444295480847359,0.010611692443490028,0.01341734454035759,0.01899005100131035,0. ... (7837 characters truncated) ... 95,-0.0029751306865364313,-0.02893241122364998,0.019140826538205147,-0.01878700964152813,0.04615134000778198,0.03211531415581703,0.03046126291155815]', 'text__4': 'Figure 1: Helpfulness human evaluation results for Llama\\n2-Chat compared to other open-source and closed-source\\nmodels. Human raters compared model ... (3476 characters truncated) ... ervations we made during the development of Llama 2 and Llama 2-Chat, such as\\nthe emergence of tool usage and temporal organization of knowledge.\\n3', 'node_id__5': '36ea0202-c5d7-484a-adfa-6e9a15b8ff15', 'metadata___5': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"4\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"36ea0202-c5d7-484a-adfa-6e9a15b8ff15\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__5': '[-0.04953619837760925,-0.008958281017839909,-0.02077280357480049,-0.0010538800852373242,0.02788402885198593,0.010536999441683292,0.010701639577746391 ... (7840 characters truncated) ... 0.008002031594514847,-0.022300947457551956,0.009685252793133259,-0.0013217638479545712,0.027760924771428108,0.023037618026137352,0.02464488334953785]', 'text__5': 'Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-\\nsource models. Human raters judged model genera ... (2247 characters truncated) ... e release of the 34B model due to a lack of time to sufficiently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4', 'node_id__6': '626bebee-27c2-4f1d-a5cd-7979107187bb', 'metadata___6': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"5\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"626bebee-27c2-4f1d-a5cd-7979107187bb\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__6': '[-0.04997732862830162,0.009823577478528023,-0.02411065623164177,0.0027358203660696745,0.0369773730635643,0.025312146171927452,-0.008218562230467796,- ... (7830 characters truncated) ... 0.005745939444750547,-0.034969910979270935,0.013228212483227253,-0.007129959762096405,0.023872578516602516,0.0022928547114133835,0.04255726933479309]', 'text__6': 'Figure 4: Training of Llama 2-Chat: This process begins with the pretraining of Llama 2 using publicly\\navailable online sources. Following this, we  ... (2561 characters truncated) ... ing rate. We use a weight decay of 0.1 and gradient clipping\\nof 1.0. Figure 5 (a) shows the training loss for Llama 2 with these hyperparameters.\\n5', 'node_id__7': '710ada03-63a4-45a0-86cc-29217dfd70bb', 'metadata___7': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"6\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"710ada03-63a4-45a0-86cc-29217dfd70bb\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__7': '[-0.02707950957119465,-0.016472401097416878,-0.013296138495206833,0.01848411187529564,0.03731360286474228,0.016059568151831627,-0.014368529431521893, ... (7836 characters truncated) ... .008573989383876324,-0.036631692200899124,0.009075476787984371,-0.030297623947262764,-0.022518446668982506,0.006921632215380669,0.046737927943468094]', 'text__7': 'Training Data\\nParams\\nContext\\nLength\\nGQA\\nTokens\\nLR\\nLlama 1\\nSee Touvron et al.\\n(2023)\\n7B\\n2k\\n✗\\n1.0T\\n3.0 × 10−4\\n13B\\n2k\\n✗\\n1.0T\\n3.0 × 10 ... (1949 characters truncated) ... suitability of these different types of\\ninterconnect for large scale training. RoCE (which is a more affordable, commercial interconnect network)\\n6', 'node_id__8': '3fd93d2d-69ae-47e4-9c1e-50611a37a5c1', 'metadata___8': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"7\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"3fd93d2d-69ae-47e4-9c1e-50611a37a5c1\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__8': '[-0.03823321685194969,0.002110098022967577,0.0291411392390728,0.008365260437130928,0.04596145451068878,0.025955475866794586,-0.03757462650537491,-0.0 ... (7807 characters truncated) ... 6,0.014288968406617641,-0.04634919390082359,0.012830430641770363,-0.006405648775398731,-0.0396643690764904,0.027757367119193077,0.051951825618743896]', 'text__8': 'Time\\n(GPU hours)\\nPower\\nConsumption (W)\\nCarbon Emitted\\n(tCO2eq)\\nLlama 2\\n7B\\n184320\\n400\\n31.22\\n13B\\n368640\\n400\\n62.44\\n34B\\n1038336\\n350\\n153 ... (3522 characters truncated) ... l., 2019).\\n• MATH. We report the average of the GSM8K (8 shot) (Cobbe et al., 2021) and MATH (4 shot)\\n(Hendrycks et al., 2021) benchmarks at top 1.', 'node_id__9': '6bb0e48a-2439-4715-ac22-a0d4e41e14df', 'metadata___9': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"7\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"6bb0e48a-2439-4715-ac22-a0d4e41e14df\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__9': '[-0.034830234944820404,-0.008186827413737774,0.0076195476576685905,-0.012563505209982395,0.04259539395570755,0.021330393850803375,-0.0128324562683701 ... (7851 characters truncated) ... ,-0.039458923041820526,-0.03477915748953819,-0.01924695074558258,-0.026713456958532333,0.006596275605261326,0.03778880089521408,0.029315859079360962]', 'text__9': 'We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks.\\n• World Knowledge. We evaluate the 5-shot performance on Nat ... (369 characters truncated) ... (Hendrycks et al., 2021) benchmarks at top 1.\\n∗∗https://sustainability.fb.com/2021-sustainability-report/\\n††https://www.mosaicml.com/blog/mpt-7b\\n7', 'node_id__10': 'c44caba6-b542-4c6e-b44e-622fbc0746fc', 'metadata___10': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"8\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"c44caba6-b542-4c6e-b44e-622fbc0746fc\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__10': '[-0.05371367931365967,-0.001831292174756527,0.002089073183014989,0.01719338819384575,0.03893434256315231,0.014204162172973156,-0.024797512218356133,0 ... (7830 characters truncated) ... ,-0.02554924413561821,-0.033042747527360916,0.018066832795739174,-0.030242621898651123,0.019609007984399796,0.024333324283361435,0.05310933291912079]', 'text__10': 'Model\\nSize\\nCode\\nCommonsense\\nReasoning\\nWorld\\nKnowledge\\nReading\\nComprehension\\nMath MMLU\\nBBH\\nAGI Eval\\nMPT\\n7B\\n20.5\\n57.4\\n41.0\\n57.5\\n4.9\\n ... (1866 characters truncated) ...  in performance between Llama 2 70B and GPT-4\\nand PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.', 'node_id__11': 'd955b954-4779-4500-aaea-5e03832a51e4', 'metadata___11': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"8\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"d955b954-4779-4500-aaea-5e03832a51e4\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__11': '[-0.02260536141693592,-0.004591614007949829,-0.029703086242079735,-0.007813162170350552,0.03561466932296753,0.010825375095009804,-0.02038831450045108 ... (7844 characters truncated) ... ,-0.03261246904730797,-0.036846574395895004,0.021556789055466652,-0.009644086472690105,0.017301740124821663,0.027997266501188278,0.06577706336975098]', 'text__11': 'Additionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed ... (1503 characters truncated) ... (GAtt), which we find helps control dialogue flow over multiple turns\\n(Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\\n8', 'node_id__12': 'bad70b0c-441a-49e7-8e0a-fc01f2d5328f', 'metadata___12': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"9\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"bad70b0c-441a-49e7-8e0a-fc01f2d5328f\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}' ... 328 parameters truncated ... 'embedding__94': '[-0.010083992965519428,0.024599460884928703,-0.02178492769598961,-0.014177397824823856,0.02642723359167576,0.03119334578514099,-0.007110936567187309, ... (7853 characters truncated) ... .016791505739092827,-0.039430633187294006,-0.016874022781848907,-0.013225521892309189,-0.004780283663421869,0.028324304148554802,0.04263126850128174]', 'text__94': 'both pretrained and fine-tuned models. The fine-tuned Llama 2-Chat shows more positivity in\\nsentiment scores than the pretrained versions do. ChatGP ... (2752 characters truncated) ... ated metrics and benchmarks in order to\\nbetter understand and analyze the varied behavior exhibited by LLMs across different demographic groups.\\n70', 'node_id__95': 'fe6f46a4-b8eb-457b-acd2-70b112b767be', 'metadata___95': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"71\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"fe6f46a4-b8eb-457b-acd2-70b112b767be\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__95': '[-0.05188710615038872,-0.010544642806053162,0.016377223655581474,0.00834600068628788,0.020156290382146835,0.059931498020887375,-0.003653642488643527, ... (7831 characters truncated) ... 0.0038335861172527075,-0.06274686008691788,-0.002220591064542532,-0.03325866162776947,-0.014156916178762913,0.014541929587721825,0.06438744813203812]', 'text__95': 'Asian\\nMexican\\nMuslim\\nPhysical\\ndisability\\nJewish\\nMiddle\\nEastern\\nChinese\\nMental\\ndisability\\nLatino\\nNative\\nAmerican\\nWomen\\nBlack\\nLGBTQ\\nPr ... (1517 characters truncated) ... 0\\n0\\n0\\n0\\n0\\n0\\n13B\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n34B\\n0.11\\n0\\n0\\n0.17\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n70B\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0.', 'node_id__96': 'f6ed639a-e933-4ebf-acbb-de58682fa3b7', 'metadata___96': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"71\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"f6ed639a-e933-4ebf-acbb-de58682fa3b7\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__96': '[-0.0005951754283159971,0.006264295428991318,0.018981609493494034,0.017994657158851624,0.06983556598424911,0.011753126978874207,-0.00772825488820672, ... (7838 characters truncated) ... ,0.021467579528689384,-0.02982889488339424,-0.010540599934756756,-0.014763440936803818,0.015629025176167488,0.036996979266405106,0.05435309559106827]', 'text__96': '94\\n12.95\\n17.94\\n11.26\\nFalcon-instruct\\n7B\\n6.23\\n9.15\\n6.02\\n7.28\\n11.19\\n6.73\\n8.01\\n7.53\\n8.61\\n8.57\\n9.05\\n7.78\\n6.46\\nLlama 2-Chat\\n7B\\n0\\n0\\n ... (1071 characters truncated) ... \\n0.39\\n70B\\n0.51\\n0.43\\n0.40\\n0.49\\nTable 46: Distribution of mean sentiment scores across groups under the race domain among the BOLD\\nprompts.\\n71', 'node_id__97': '226cd066-dcc6-4094-8040-ab9da50ec4c0', 'metadata___97': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"72\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"226cd066-dcc6-4094-8040-ab9da50ec4c0\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__97': '[-0.03292449936270714,0.009152770973742008,0.0016094630118459463,-0.03403041511774063,0.014729073271155357,0.03696044161915779,0.02417147532105446,0. ... (7834 characters truncated) ... 0.012803185731172562,-0.018036488443613052,-0.004750636871904135,-0.013044239953160286,-0.024809129536151886,0.04458576440811157,0.04801412299275398]', 'text__97': 'American actors\\nAmerican actresses\\nPretrained\\nMPT\\n7B\\n0.30\\n0.43\\n30B\\n0.29\\n0.41\\nFalcon\\n7B\\n0.21\\n0.33\\n40B\\n0.29\\n0.37\\nLlama 1\\n7B\\n0.31\\n0. ... (2305 characters truncated) ... ative user experiences and shared these categories and examples with the annotators. A summary of\\nthese categories can be seen in Section A.5.2.\\n72', 'node_id__98': '231e5eba-3549-4b8b-8a7a-da4660a6e2eb', 'metadata___98': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"73\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"231e5eba-3549-4b8b-8a7a-da4660a6e2eb\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__98': '[0.0015203604707494378,0.025429649278521538,-0.026538928970694542,-0.020361701026558876,0.04127050191164017,0.04644489660859108,-0.005709849763661623 ... (7830 characters truncated) ... 7,0.009693905711174011,-0.052033793181180954,-0.018948974087834358,-0.02575545385479927,0.01945745199918747,0.010145256295800209,0.05104433000087738]', 'text__98': 'Judaism\\nChristianity\\nIslam\\nBuddhism\\nSikhism\\nPretrained\\nMPT\\n7B\\n0.39\\n0.38\\n0.31\\n0.27\\n0.07\\n30B\\n0.33\\n0.28\\n0.20\\n0.30\\n0.19\\nFalcon\\n7B\\n0. ... (1478 characters truncated) ... n0.09\\n0.29\\n-0.26\\n65B\\n0.11\\n0.20\\n0.27\\n0.35\\n0.31\\n0.52\\n0.21\\n0.59\\n0.25\\n0.19\\n0.33\\n-0.25\\nLlama 2\\n7B\\n0.15\\n0.30\\n0.12\\n0.35\\n0.25\\n0.43\\n0.', 'node_id__99': 'a3b93397-4b39-4830-bf66-298bdc2d496b', 'metadata___99': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"73\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"a3b93397-4b39-4830-bf66-298bdc2d496b\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__99': '[-0.05732172727584839,-0.0074186199344694614,0.01727089285850525,-0.012951971031725407,0.03807681053876877,-0.013952450826764107,-0.03538889810442924 ... (7827 characters truncated) ... 0.0074768043123185635,-0.03725611791014671,0.011675733141601086,-0.050115782767534256,0.03690603747963905,-0.016451453790068626,0.057954851537942886]', 'text__99': '20\\n-0.23\\n13B\\n0.18\\n0.09\\n0.26\\n0.29\\n0.26\\n0.53\\n0.10\\n0.49\\n0.20\\n0.16\\n0.15\\n-0.21\\n33B\\n0.22\\n0.18\\n0.26\\n0.27\\n0.28\\n0.50\\n0.06\\n0.55\\n0.26\\n0 ... (993 characters truncated) ... .55\\n0.34\\n0.57\\n-0.01\\nTable 49: Distribution of mean sentiment scores across groups under the political ideology domain from the\\nBOLD prompts.\\n73', 'node_id__100': '5fb1a34e-0590-422a-984e-e235df3c2e8f', 'metadata___100': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"74\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"5fb1a34e-0590-422a-984e-e235df3c2e8f\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__100': '[-0.05010180175304413,0.0021021857392042875,0.04283490404486656,0.0025432913098484278,0.03663600981235504,0.02941935323178768,-0.0037676456850022078, ... (7839 characters truncated) ... 5,0.015147608704864979,-0.04166238754987717,0.006707045715302229,-0.04191281273961067,-0.008607772178947926,0.035873934626579285,0.04929667338728905]', 'text__100': 'Metal-\\nworking\\nSewing\\nHealthcare\\nComputer\\nFilm &\\ntelevision\\nArtistic\\nScientific\\nEntertainer\\nDance\\nNursing\\nspecialties\\nWriting\\nProfessio ... (1433 characters truncated) ... 0.51\\n0.41\\n0.45\\n0.27\\n0.34\\n0.40\\n0.52\\n0.36\\n0.12\\n0.28\\n0.31\\n0.45\\n0.65\\n0.33\\n0.20\\nFine-tuned\\nChatGPT\\n0.65\\n0.62\\n0.64\\n0.84\\n0.77\\n0.75\\n0.', 'node_id__101': '3a4f79c4-ff88-41a2-9941-a6ec575e4d93', 'metadata___101': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"74\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"3a4f79c4-ff88-41a2-9941-a6ec575e4d93\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__101': '[-0.0492093600332737,0.002657530130818486,0.008977214805781841,-0.029528671875596046,0.02765767276287079,-0.0049738758243620396,-0.030532479286193848 ... (7846 characters truncated) ... 7,-0.015748830512166023,-0.04249212145805359,0.016864409670233727,-0.05668102577328682,0.04663722589612007,-0.009948094375431538,0.04124179482460022]', 'text__101': '46\\n0.66\\n0.35\\n0.28\\n34B\\n0.27\\n0.24\\n0.33\\n0.56\\n0.41\\n0.36\\n0.26\\n0.32\\n0.36\\n0.53\\n0.33\\n0.07\\n0.26\\n0.30\\n0.45\\n0.56\\n0.26\\n0.35\\n70B\\n0.31\\n0.2 ... (2136 characters truncated) ... e prompt (c) are\\nfree of grammatical, spelling and other writing errors, and (d) do not fall into any of the categories described\\nin Section A.5.2.', 'node_id__102': 'd6cb2270-3f72-43aa-95ee-b1080a9b6902', 'metadata___102': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"74\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"d6cb2270-3f72-43aa-95ee-b1080a9b6902\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__102': '[-0.03945086896419525,0.02313345856964588,-0.0038420879282057285,-0.02819398045539856,0.019531410187482834,0.03562150523066521,0.007483062334358692,0 ... (7796 characters truncated) ... .00023599402629770339,0.0041661751456558704,-0.023356640711426735,-0.010518582537770271,0.02797902189195156,0.07357597351074219,0.038634203374385834]', 'text__102': 'Promote or enable criminal activities.\\n2. Promote or enable dangerous behaviors to the user or other people.\\n3. Contain, promote or enable offensiv ... (2010 characters truncated) ... To pass the test, annotators\\nneeded to agree with our criteria on 80% of the answers, and pass the written examples with a score\\nof 4 out of 5.\\n74', 'node_id__103': 'f5233073-70dd-4f0b-9fda-bf6483028f83', 'metadata___103': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"75\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"f5233073-70dd-4f0b-9fda-bf6483028f83\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__103': '[-0.04160007834434509,0.0042624580673873425,-0.021666696295142174,-0.006299999542534351,0.05985827371478081,0.006744795944541693,0.012321275658905506 ... (7824 characters truncated) ... 56845,-0.03271615505218506,-0.014986553229391575,0.0108970757573843,-0.0343596525490284,0.01650281809270382,0.02128870226442814,0.013972227461636066]', 'text__103': '• The third test consisted in measuring the alignment with our quality assessment criteria. The test\\nconsisted of 31 different questions asking the  ... (4168 characters truncated) ... amples to have an overall better\\naverage score than their complement. It is insufficient evidence for contamination if only one of these were\\ntrue.', 'node_id__104': '35c5d2b4-572b-4d41-a75f-3b52eef387b6', 'metadata___104': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"75\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"35c5d2b4-572b-4d41-a75f-3b52eef387b6\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__104': '[-0.014338103123009205,-0.006474675145000219,-3.892310633091256e-05,-0.002099551958963275,0.04757043346762657,-0.0032614206429570913,0.02705727517604 ... (7824 characters truncated) ... 09751430712640285,-0.005034156609326601,0.006561452057212591,-0.025418641045689583,-0.012639285996556282,-0.005089836660772562,-0.009446755051612854]', 'text__104': '(2022), modified to work on a PySpark cluster (effectively without random access to disk). Given the\\nembarrassingly parallel nature of the task, we  ... (1246 characters truncated) ... tly. With the given definition of\\ncontamination (as well as other definitions mentioned in the literature), there is a possibility that a sample\\n75', 'node_id__105': '4816c102-22bb-4d7f-9237-b223573d4e54', 'metadata___105': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"76\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"4816c102-22bb-4d7f-9237-b223573d4e54\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__105': '[-0.024945102632045746,0.00010394136188551784,-0.002446508500725031,-0.00953178945928812,0.07095226645469666,-0.016237301751971245,-0.006012015976011 ... (7857 characters truncated) ... 798998801968992,0.0008685553329996765,0.015512287616729736,-0.021013597026467323,-0.034423913806676865,-0.00042012488120235503,-0.009545068256556988]', 'text__105': 'Dataset\\nModel\\nSubset Type\\nAvg. Contam. %\\nn\\n¯X\\nµn\\nZn\\nHellaSwag (L = 40)\\n70B\\nClean\\n0\\n7391\\n80.0\\n82.5\\n-5.73\\nNot Clean\\n67.5\\n2651\\n89.5\\n ... (2878 characters truncated) ... r dataset (for any\\nchoice of L) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76', 'node_id__106': '9000b2c9-0220-4e1c-b703-1d0e9462415b', 'metadata___106': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"77\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"9000b2c9-0220-4e1c-b703-1d0e9462415b\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__106': '[-0.047984685748815536,-0.011453934945166111,-0.007028207182884216,0.0034323756117373705,0.02276965230703354,0.018554989248514175,-0.0140381306409835 ... (7824 characters truncated) ... ,-0.009772544726729393,-0.03153320774435997,0.02807709388434887,-0.007342176977545023,0.0007125982665456831,0.02753782831132412,0.044340990483760834]', 'text__106': 'A.7\\nModel Card\\nTable 52 presents a model card (Mitchell et al., 2018; Anil et al., 2023) that summarizes details of the models.\\nModel Details\\nMod ... (3240 characters truncated) ...  Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card for Llama 2.\\n77'}]\n(Background on this error at: https://sqlalche.me/e/20/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedTable\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2115\u001b[39m, in \u001b[36mConnection._exec_insertmany_context\u001b[39m\u001b[34m(self, dialect, context)\u001b[39m\n\u001b[32m   2114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2115\u001b[39m         \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m            \u001b[49m\u001b[43msub_stmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m            \u001b[49m\u001b[43msub_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py:942\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mUndefinedTable\u001b[39m: relation \"public.data_llama2_paper\" does not exist\nLINE 1: INSERT INTO public.data_llama2_paper (text, metadata_, node_...\n                    ^\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Store Nodes in the Vector Store\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNodes have been added to the vector store!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/llama_index/vector_stores/postgres/base.py:492\u001b[39m, in \u001b[36mPGVectorStore.add\u001b[39m\u001b[34m(self, nodes, **add_kwargs)\u001b[39m\n\u001b[32m    490\u001b[39m         item = \u001b[38;5;28mself\u001b[39m._node_to_table_row(node)\n\u001b[32m    491\u001b[39m         session.add(item)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2032\u001b[39m, in \u001b[36mSession.commit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trans \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2030\u001b[39m     trans = \u001b[38;5;28mself\u001b[39m._autobegin_t()\n\u001b[32m-> \u001b[39m\u001b[32m2032\u001b[39m \u001b[43mtrans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_root\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36mcommit\u001b[39m\u001b[34m(self, _to_root)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/state_changes.py:139\u001b[39m, in \u001b[36m_StateChange.declare_states.<locals>._go\u001b[39m\u001b[34m(fn, self, *arg, **kw)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m._next_state = _StateChangeStates.CHANGE_IN_PROGRESS\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     ret_value = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py:1313\u001b[39m, in \u001b[36mSessionTransaction.commit\u001b[39m\u001b[34m(self, _to_root)\u001b[39m\n\u001b[32m   1311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SessionTransactionState.PREPARED:\n\u001b[32m   1312\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expect_state(SessionTransactionState.PREPARED):\n\u001b[32m-> \u001b[39m\u001b[32m1313\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nested:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m conn, trans, should_commit, autoclose \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\n\u001b[32m   1317\u001b[39m         \u001b[38;5;28mself\u001b[39m._connections.values()\n\u001b[32m   1318\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36m_prepare_impl\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/state_changes.py:139\u001b[39m, in \u001b[36m_StateChange.declare_states.<locals>._go\u001b[39m\u001b[34m(fn, self, *arg, **kw)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m._next_state = _StateChangeStates.CHANGE_IN_PROGRESS\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     ret_value = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py:1288\u001b[39m, in \u001b[36mSessionTransaction._prepare_impl\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.session._is_clean():\n\u001b[32m   1287\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.FlushError(\n\u001b[32m   1291\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOver 100 subsequent flushes have occurred within \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1292\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msession.commit() - is an after_flush() hook \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1293\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcreating new objects?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1294\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py:4353\u001b[39m, in \u001b[36mSession.flush\u001b[39m\u001b[34m(self, objects)\u001b[39m\n\u001b[32m   4351\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   4352\u001b[39m     \u001b[38;5;28mself\u001b[39m._flushing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4353\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4354\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   4355\u001b[39m     \u001b[38;5;28mself\u001b[39m._flushing = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py:4488\u001b[39m, in \u001b[36mSession._flush\u001b[39m\u001b[34m(self, objects)\u001b[39m\n\u001b[32m   4485\u001b[39m     transaction.commit()\n\u001b[32m   4487\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4488\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   4489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransaction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_capture_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py:146\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py:4449\u001b[39m, in \u001b[36mSession._flush\u001b[39m\u001b[34m(self, objects)\u001b[39m\n\u001b[32m   4447\u001b[39m \u001b[38;5;28mself\u001b[39m._warn_on_events = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   4448\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4449\u001b[39m     \u001b[43mflush_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4450\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   4451\u001b[39m     \u001b[38;5;28mself\u001b[39m._warn_on_events = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py:466\u001b[39m, in \u001b[36mUOWTransaction.execute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m topological.sort(\u001b[38;5;28mself\u001b[39m.dependencies, postsort_actions):\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m         \u001b[43mrec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py:642\u001b[39m, in \u001b[36mSaveUpdateAll.execute\u001b[39m\u001b[34m(self, uow)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;129m@util\u001b[39m.preload_module(\u001b[33m\"\u001b[39m\u001b[33msqlalchemy.orm.persistence\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, uow):\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m     \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreloaded\u001b[49m\u001b[43m.\u001b[49m\u001b[43morm_persistence\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43muow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstates_for_mapper_hierarchy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43muow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py:93\u001b[39m, in \u001b[36msave_obj\u001b[39m\u001b[34m(base_mapper, states, uowtransaction, single)\u001b[39m\n\u001b[32m     81\u001b[39m     update = _collect_update_commands(\n\u001b[32m     82\u001b[39m         uowtransaction, table, states_to_update\n\u001b[32m     83\u001b[39m     )\n\u001b[32m     85\u001b[39m     _emit_update_statements(\n\u001b[32m     86\u001b[39m         base_mapper,\n\u001b[32m     87\u001b[39m         uowtransaction,\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m         update,\n\u001b[32m     91\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[43m_emit_insert_statements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43muowtransaction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43minsert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m _finalize_insert_update_commands(\n\u001b[32m    102\u001b[39m     base_mapper,\n\u001b[32m    103\u001b[39m     uowtransaction,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m     ),\n\u001b[32m    120\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py:1143\u001b[39m, in \u001b[36m_emit_insert_statements\u001b[39m\u001b[34m(base_mapper, uowtransaction, mapper, table, insert, bookkeeping, use_orm_insert_stmt, execution_options)\u001b[39m\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_executemany:\n\u001b[32m   1141\u001b[39m     multiparams = [rec[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m records]\n\u001b[32m-> \u001b[39m\u001b[32m1143\u001b[39m     result = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_orm_insert_stmt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m return_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1416\u001b[39m, in \u001b[36mConnection.execute\u001b[39m\u001b[34m(self, statement, parameters, execution_options)\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1417\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:523\u001b[39m, in \u001b[36mClauseElement._execute_on_connection\u001b[39m\u001b[34m(self, connection, distilled_params, execution_options)\u001b[39m\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m    522\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1638\u001b[39m, in \u001b[36mConnection._execute_clauseelement\u001b[39m\u001b[34m(self, elem, distilled_parameters, execution_options)\u001b[39m\n\u001b[32m   1626\u001b[39m compiled_cache: Optional[CompiledCacheType] = execution_options.get(\n\u001b[32m   1627\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompiled_cache\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.engine._compiled_cache\n\u001b[32m   1628\u001b[39m )\n\u001b[32m   1630\u001b[39m compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(\n\u001b[32m   1631\u001b[39m     dialect=dialect,\n\u001b[32m   1632\u001b[39m     compiled_cache=compiled_cache,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1636\u001b[39m     linting=\u001b[38;5;28mself\u001b[39m.dialect.compiler_linting | compiler.WARN_LINTING,\n\u001b[32m   1637\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1638\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[32m   1651\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_execute(\n\u001b[32m   1652\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1653\u001b[39m         elem,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1657\u001b[39m         ret,\n\u001b[32m   1658\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1841\u001b[39m, in \u001b[36mConnection._execute_context\u001b[39m\u001b[34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[39m\n\u001b[32m   1838\u001b[39m context.pre_exec()\n\u001b[32m   1840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.execute_style \u001b[38;5;129;01mis\u001b[39;00m ExecuteStyle.INSERTMANYVALUES:\n\u001b[32m-> \u001b[39m\u001b[32m1841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_exec_insertmany_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exec_single_context(\n\u001b[32m   1844\u001b[39m         dialect, context, statement, parameters\n\u001b[32m   1845\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2123\u001b[39m, in \u001b[36mConnection._exec_insertmany_context\u001b[39m\u001b[34m(self, dialect, context)\u001b[39m\n\u001b[32m   2115\u001b[39m         dialect.do_execute(\n\u001b[32m   2116\u001b[39m             cursor,\n\u001b[32m   2117\u001b[39m             sub_stmt,\n\u001b[32m   2118\u001b[39m             sub_params,\n\u001b[32m   2119\u001b[39m             context,\n\u001b[32m   2120\u001b[39m         )\n\u001b[32m   2122\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2123\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_long_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_stmt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m        \u001b[49m\u001b[43msub_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_sub_exec\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2130\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine_events:\n\u001b[32m   2133\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_cursor_execute(\n\u001b[32m   2134\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2135\u001b[39m         cursor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2139\u001b[39m         context.executemany,\n\u001b[32m   2140\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2352\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception\u001b[39m\u001b[34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[39m\n\u001b[32m   2350\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2351\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2352\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2354\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2115\u001b[39m, in \u001b[36mConnection._exec_insertmany_context\u001b[39m\u001b[34m(self, dialect, context)\u001b[39m\n\u001b[32m   2113\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   2114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2115\u001b[39m         \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m            \u001b[49m\u001b[43msub_stmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m            \u001b[49m\u001b[43msub_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2123\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_dbapi_exception(\n\u001b[32m   2124\u001b[39m         e,\n\u001b[32m   2125\u001b[39m         sql_util._long_statement(sub_stmt),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2129\u001b[39m         is_sub_exec=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2130\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py:942\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mProgrammingError\u001b[39m: (psycopg2.errors.UndefinedTable) relation \"public.data_llama2_paper\" does not exist\nLINE 1: INSERT INTO public.data_llama2_paper (text, metadata_, node_...\n                    ^\n\n[SQL: INSERT INTO public.data_llama2_paper (text, metadata_, node_id, embedding) SELECT p0::VARCHAR, p1::JSON, p2::VARCHAR, p3::VECTOR(384) FROM (VALUES (%(text__0)s, %(metadata___0)s::JSON, %(node_id__0)s, %(embedding__0)s, 0), (%(text__1)s, %(metadata___ ... 8585 characters truncated ... er) ORDER BY sen_counter RETURNING public.data_llama2_paper.id, public.data_llama2_paper.id AS id__1]\n[parameters: {'node_id__0': 'cbfc2660-d930-4a25-967e-15cf5a4f17a1', 'metadata___0': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"1\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"cbfc2660-d930-4a25-967e-15cf5a4f17a1\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__0': '[-0.04617450758814812,0.03491324186325073,-0.013553637079894543,-0.007586942054331303,0.017399096861481667,-0.00029551811167038977,-0.008765399456024 ... (7818 characters truncated) ... ,0.008555163629353046,-0.017789412289857864,0.004046207293868065,-0.029435161501169205,0.00034099709591828287,0.02046983316540718,0.0345381461083889]', 'text__0': 'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗\\nLouis Martin†\\nKevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay  ... (1626 characters truncated) ... scialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023', 'node_id__1': '3c480d46-9ad5-4048-a605-7768eff38c60', 'metadata___1': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"2\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"3c480d46-9ad5-4048-a605-7768eff38c60\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__1': '[-0.070559062063694,-0.02465185895562172,0.02275167964398861,-0.019947784021496773,0.020725281909108162,0.03974338248372078,-0.02047322317957878,0.00 ... (7838 characters truncated) ... -0.013566997833549976,-0.051573220640420914,0.027146410197019577,-0.04085184633731842,0.04744908958673477,0.013965633697807789,0.0020637046545743942]', 'text__1': 'Contents\\n1\\nIntroduction\\n3\\n2\\nPretraining\\n5\\n2.1\\nPretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ... (1093 characters truncated) ... ma 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n5\\nDiscussion\\n32\\n5.1\\nLearnings and Observations . . . . . . . .', 'node_id__2': '4416fde3-2678-44f4-acbf-2aa8d8802a8f', 'metadata___2': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"2\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"4416fde3-2678-44f4-acbf-2aa8d8802a8f\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__2': '[-0.05603757128119469,-0.01130712777376175,0.009145677089691162,-0.029648946598172188,0.053696755319833755,0.016262132674455643,-0.02239496260881424, ... (7826 characters truncated) ... 6,-0.012882216833531857,-0.04809080436825752,0.013987668789923191,-0.03842881694436073,0.01268172636628151,0.021383190527558327,0.021790459752082825]', 'text__2': '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n4.4\\nSafety Evaluation of Llama 2-Chat . . . . . . . . . . ... (1075 characters truncated) ...  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n75\\nA.7 Model Card\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .', 'node_id__3': 'f1124af1-7c62-4f44-8008-a74001c612af', 'metadata___3': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"2\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"f1124af1-7c62-4f44-8008-a74001c612af\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__3': '[-0.04469098523259163,-0.03675113990902901,0.031259194016456604,-0.00794018805027008,0.03586065024137497,0.0026993893552571535,-0.029911573976278305, ... (7826 characters truncated) ... -0.02732889913022518,-0.023450074717402458,0.018889112398028374,-0.03715568780899048,0.012513435445725918,-0.004459341522306204,0.006109000649303198]', 'text__3': '. . . . . . . . . . . . . . . . . . . . . . . .\\n72\\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n75\\nA.7 Model Card\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n2', 'node_id__4': '93f7bdb9-e0c7-4f47-a115-fcf64051ae26', 'metadata___4': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"3\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"93f7bdb9-e0c7-4f47-a115-fcf64051ae26\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__4': '[-0.030008263885974884,0.007953417487442493,0.015078099444508553,0.015444295480847359,0.010611692443490028,0.01341734454035759,0.01899005100131035,0. ... (7837 characters truncated) ... 95,-0.0029751306865364313,-0.02893241122364998,0.019140826538205147,-0.01878700964152813,0.04615134000778198,0.03211531415581703,0.03046126291155815]', 'text__4': 'Figure 1: Helpfulness human evaluation results for Llama\\n2-Chat compared to other open-source and closed-source\\nmodels. Human raters compared model ... (3476 characters truncated) ... ervations we made during the development of Llama 2 and Llama 2-Chat, such as\\nthe emergence of tool usage and temporal organization of knowledge.\\n3', 'node_id__5': '36ea0202-c5d7-484a-adfa-6e9a15b8ff15', 'metadata___5': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"4\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"36ea0202-c5d7-484a-adfa-6e9a15b8ff15\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__5': '[-0.04953619837760925,-0.008958281017839909,-0.02077280357480049,-0.0010538800852373242,0.02788402885198593,0.010536999441683292,0.010701639577746391 ... (7840 characters truncated) ... 0.008002031594514847,-0.022300947457551956,0.009685252793133259,-0.0013217638479545712,0.027760924771428108,0.023037618026137352,0.02464488334953785]', 'text__5': 'Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-\\nsource models. Human raters judged model genera ... (2247 characters truncated) ... e release of the 34B model due to a lack of time to sufficiently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4', 'node_id__6': '626bebee-27c2-4f1d-a5cd-7979107187bb', 'metadata___6': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"5\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"626bebee-27c2-4f1d-a5cd-7979107187bb\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__6': '[-0.04997732862830162,0.009823577478528023,-0.02411065623164177,0.0027358203660696745,0.0369773730635643,0.025312146171927452,-0.008218562230467796,- ... (7830 characters truncated) ... 0.005745939444750547,-0.034969910979270935,0.013228212483227253,-0.007129959762096405,0.023872578516602516,0.0022928547114133835,0.04255726933479309]', 'text__6': 'Figure 4: Training of Llama 2-Chat: This process begins with the pretraining of Llama 2 using publicly\\navailable online sources. Following this, we  ... (2561 characters truncated) ... ing rate. We use a weight decay of 0.1 and gradient clipping\\nof 1.0. Figure 5 (a) shows the training loss for Llama 2 with these hyperparameters.\\n5', 'node_id__7': '710ada03-63a4-45a0-86cc-29217dfd70bb', 'metadata___7': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"6\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"710ada03-63a4-45a0-86cc-29217dfd70bb\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__7': '[-0.02707950957119465,-0.016472401097416878,-0.013296138495206833,0.01848411187529564,0.03731360286474228,0.016059568151831627,-0.014368529431521893, ... (7836 characters truncated) ... .008573989383876324,-0.036631692200899124,0.009075476787984371,-0.030297623947262764,-0.022518446668982506,0.006921632215380669,0.046737927943468094]', 'text__7': 'Training Data\\nParams\\nContext\\nLength\\nGQA\\nTokens\\nLR\\nLlama 1\\nSee Touvron et al.\\n(2023)\\n7B\\n2k\\n✗\\n1.0T\\n3.0 × 10−4\\n13B\\n2k\\n✗\\n1.0T\\n3.0 × 10 ... (1949 characters truncated) ... suitability of these different types of\\ninterconnect for large scale training. RoCE (which is a more affordable, commercial interconnect network)\\n6', 'node_id__8': '3fd93d2d-69ae-47e4-9c1e-50611a37a5c1', 'metadata___8': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"7\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"3fd93d2d-69ae-47e4-9c1e-50611a37a5c1\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__8': '[-0.03823321685194969,0.002110098022967577,0.0291411392390728,0.008365260437130928,0.04596145451068878,0.025955475866794586,-0.03757462650537491,-0.0 ... (7807 characters truncated) ... 6,0.014288968406617641,-0.04634919390082359,0.012830430641770363,-0.006405648775398731,-0.0396643690764904,0.027757367119193077,0.051951825618743896]', 'text__8': 'Time\\n(GPU hours)\\nPower\\nConsumption (W)\\nCarbon Emitted\\n(tCO2eq)\\nLlama 2\\n7B\\n184320\\n400\\n31.22\\n13B\\n368640\\n400\\n62.44\\n34B\\n1038336\\n350\\n153 ... (3522 characters truncated) ... l., 2019).\\n• MATH. We report the average of the GSM8K (8 shot) (Cobbe et al., 2021) and MATH (4 shot)\\n(Hendrycks et al., 2021) benchmarks at top 1.', 'node_id__9': '6bb0e48a-2439-4715-ac22-a0d4e41e14df', 'metadata___9': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"7\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"6bb0e48a-2439-4715-ac22-a0d4e41e14df\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__9': '[-0.034830234944820404,-0.008186827413737774,0.0076195476576685905,-0.012563505209982395,0.04259539395570755,0.021330393850803375,-0.0128324562683701 ... (7851 characters truncated) ... ,-0.039458923041820526,-0.03477915748953819,-0.01924695074558258,-0.026713456958532333,0.006596275605261326,0.03778880089521408,0.029315859079360962]', 'text__9': 'We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks.\\n• World Knowledge. We evaluate the 5-shot performance on Nat ... (369 characters truncated) ... (Hendrycks et al., 2021) benchmarks at top 1.\\n∗∗https://sustainability.fb.com/2021-sustainability-report/\\n††https://www.mosaicml.com/blog/mpt-7b\\n7', 'node_id__10': 'c44caba6-b542-4c6e-b44e-622fbc0746fc', 'metadata___10': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"8\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"c44caba6-b542-4c6e-b44e-622fbc0746fc\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__10': '[-0.05371367931365967,-0.001831292174756527,0.002089073183014989,0.01719338819384575,0.03893434256315231,0.014204162172973156,-0.024797512218356133,0 ... (7830 characters truncated) ... ,-0.02554924413561821,-0.033042747527360916,0.018066832795739174,-0.030242621898651123,0.019609007984399796,0.024333324283361435,0.05310933291912079]', 'text__10': 'Model\\nSize\\nCode\\nCommonsense\\nReasoning\\nWorld\\nKnowledge\\nReading\\nComprehension\\nMath MMLU\\nBBH\\nAGI Eval\\nMPT\\n7B\\n20.5\\n57.4\\n41.0\\n57.5\\n4.9\\n ... (1866 characters truncated) ...  in performance between Llama 2 70B and GPT-4\\nand PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.', 'node_id__11': 'd955b954-4779-4500-aaea-5e03832a51e4', 'metadata___11': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"8\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"d955b954-4779-4500-aaea-5e03832a51e4\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__11': '[-0.02260536141693592,-0.004591614007949829,-0.029703086242079735,-0.007813162170350552,0.03561466932296753,0.010825375095009804,-0.02038831450045108 ... (7844 characters truncated) ... ,-0.03261246904730797,-0.036846574395895004,0.021556789055466652,-0.009644086472690105,0.017301740124821663,0.027997266501188278,0.06577706336975098]', 'text__11': 'Additionally, Llama 2 70B model outperforms all open-source models.\\nIn addition to open-source models, we also compare Llama 2 70B results to closed ... (1503 characters truncated) ... (GAtt), which we find helps control dialogue flow over multiple turns\\n(Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\\n8', 'node_id__12': 'bad70b0c-441a-49e7-8e0a-fc01f2d5328f', 'metadata___12': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"9\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"bad70b0c-441a-49e7-8e0a-fc01f2d5328f\\\\\", \\\\\"emb ... (496 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}' ... 328 parameters truncated ... 'embedding__94': '[-0.010083992965519428,0.024599460884928703,-0.02178492769598961,-0.014177397824823856,0.02642723359167576,0.03119334578514099,-0.007110936567187309, ... (7853 characters truncated) ... .016791505739092827,-0.039430633187294006,-0.016874022781848907,-0.013225521892309189,-0.004780283663421869,0.028324304148554802,0.04263126850128174]', 'text__94': 'both pretrained and fine-tuned models. The fine-tuned Llama 2-Chat shows more positivity in\\nsentiment scores than the pretrained versions do. ChatGP ... (2752 characters truncated) ... ated metrics and benchmarks in order to\\nbetter understand and analyze the varied behavior exhibited by LLMs across different demographic groups.\\n70', 'node_id__95': 'fe6f46a4-b8eb-457b-acd2-70b112b767be', 'metadata___95': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"71\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"fe6f46a4-b8eb-457b-acd2-70b112b767be\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__95': '[-0.05188710615038872,-0.010544642806053162,0.016377223655581474,0.00834600068628788,0.020156290382146835,0.059931498020887375,-0.003653642488643527, ... (7831 characters truncated) ... 0.0038335861172527075,-0.06274686008691788,-0.002220591064542532,-0.03325866162776947,-0.014156916178762913,0.014541929587721825,0.06438744813203812]', 'text__95': 'Asian\\nMexican\\nMuslim\\nPhysical\\ndisability\\nJewish\\nMiddle\\nEastern\\nChinese\\nMental\\ndisability\\nLatino\\nNative\\nAmerican\\nWomen\\nBlack\\nLGBTQ\\nPr ... (1517 characters truncated) ... 0\\n0\\n0\\n0\\n0\\n0\\n13B\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n34B\\n0.11\\n0\\n0\\n0.17\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n70B\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0.', 'node_id__96': 'f6ed639a-e933-4ebf-acbb-de58682fa3b7', 'metadata___96': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"71\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"f6ed639a-e933-4ebf-acbb-de58682fa3b7\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__96': '[-0.0005951754283159971,0.006264295428991318,0.018981609493494034,0.017994657158851624,0.06983556598424911,0.011753126978874207,-0.00772825488820672, ... (7838 characters truncated) ... ,0.021467579528689384,-0.02982889488339424,-0.010540599934756756,-0.014763440936803818,0.015629025176167488,0.036996979266405106,0.05435309559106827]', 'text__96': '94\\n12.95\\n17.94\\n11.26\\nFalcon-instruct\\n7B\\n6.23\\n9.15\\n6.02\\n7.28\\n11.19\\n6.73\\n8.01\\n7.53\\n8.61\\n8.57\\n9.05\\n7.78\\n6.46\\nLlama 2-Chat\\n7B\\n0\\n0\\n ... (1071 characters truncated) ... \\n0.39\\n70B\\n0.51\\n0.43\\n0.40\\n0.49\\nTable 46: Distribution of mean sentiment scores across groups under the race domain among the BOLD\\nprompts.\\n71', 'node_id__97': '226cd066-dcc6-4094-8040-ab9da50ec4c0', 'metadata___97': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"72\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"226cd066-dcc6-4094-8040-ab9da50ec4c0\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__97': '[-0.03292449936270714,0.009152770973742008,0.0016094630118459463,-0.03403041511774063,0.014729073271155357,0.03696044161915779,0.02417147532105446,0. ... (7834 characters truncated) ... 0.012803185731172562,-0.018036488443613052,-0.004750636871904135,-0.013044239953160286,-0.024809129536151886,0.04458576440811157,0.04801412299275398]', 'text__97': 'American actors\\nAmerican actresses\\nPretrained\\nMPT\\n7B\\n0.30\\n0.43\\n30B\\n0.29\\n0.41\\nFalcon\\n7B\\n0.21\\n0.33\\n40B\\n0.29\\n0.37\\nLlama 1\\n7B\\n0.31\\n0. ... (2305 characters truncated) ... ative user experiences and shared these categories and examples with the annotators. A summary of\\nthese categories can be seen in Section A.5.2.\\n72', 'node_id__98': '231e5eba-3549-4b8b-8a7a-da4660a6e2eb', 'metadata___98': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"73\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"231e5eba-3549-4b8b-8a7a-da4660a6e2eb\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__98': '[0.0015203604707494378,0.025429649278521538,-0.026538928970694542,-0.020361701026558876,0.04127050191164017,0.04644489660859108,-0.005709849763661623 ... (7830 characters truncated) ... 7,0.009693905711174011,-0.052033793181180954,-0.018948974087834358,-0.02575545385479927,0.01945745199918747,0.010145256295800209,0.05104433000087738]', 'text__98': 'Judaism\\nChristianity\\nIslam\\nBuddhism\\nSikhism\\nPretrained\\nMPT\\n7B\\n0.39\\n0.38\\n0.31\\n0.27\\n0.07\\n30B\\n0.33\\n0.28\\n0.20\\n0.30\\n0.19\\nFalcon\\n7B\\n0. ... (1478 characters truncated) ... n0.09\\n0.29\\n-0.26\\n65B\\n0.11\\n0.20\\n0.27\\n0.35\\n0.31\\n0.52\\n0.21\\n0.59\\n0.25\\n0.19\\n0.33\\n-0.25\\nLlama 2\\n7B\\n0.15\\n0.30\\n0.12\\n0.35\\n0.25\\n0.43\\n0.', 'node_id__99': 'a3b93397-4b39-4830-bf66-298bdc2d496b', 'metadata___99': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"73\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"a3b93397-4b39-4830-bf66-298bdc2d496b\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__99': '[-0.05732172727584839,-0.0074186199344694614,0.01727089285850525,-0.012951971031725407,0.03807681053876877,-0.013952450826764107,-0.03538889810442924 ... (7827 characters truncated) ... 0.0074768043123185635,-0.03725611791014671,0.011675733141601086,-0.050115782767534256,0.03690603747963905,-0.016451453790068626,0.057954851537942886]', 'text__99': '20\\n-0.23\\n13B\\n0.18\\n0.09\\n0.26\\n0.29\\n0.26\\n0.53\\n0.10\\n0.49\\n0.20\\n0.16\\n0.15\\n-0.21\\n33B\\n0.22\\n0.18\\n0.26\\n0.27\\n0.28\\n0.50\\n0.06\\n0.55\\n0.26\\n0 ... (993 characters truncated) ... .55\\n0.34\\n0.57\\n-0.01\\nTable 49: Distribution of mean sentiment scores across groups under the political ideology domain from the\\nBOLD prompts.\\n73', 'node_id__100': '5fb1a34e-0590-422a-984e-e235df3c2e8f', 'metadata___100': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"74\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"5fb1a34e-0590-422a-984e-e235df3c2e8f\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__100': '[-0.05010180175304413,0.0021021857392042875,0.04283490404486656,0.0025432913098484278,0.03663600981235504,0.02941935323178768,-0.0037676456850022078, ... (7839 characters truncated) ... 5,0.015147608704864979,-0.04166238754987717,0.006707045715302229,-0.04191281273961067,-0.008607772178947926,0.035873934626579285,0.04929667338728905]', 'text__100': 'Metal-\\nworking\\nSewing\\nHealthcare\\nComputer\\nFilm &\\ntelevision\\nArtistic\\nScientific\\nEntertainer\\nDance\\nNursing\\nspecialties\\nWriting\\nProfessio ... (1433 characters truncated) ... 0.51\\n0.41\\n0.45\\n0.27\\n0.34\\n0.40\\n0.52\\n0.36\\n0.12\\n0.28\\n0.31\\n0.45\\n0.65\\n0.33\\n0.20\\nFine-tuned\\nChatGPT\\n0.65\\n0.62\\n0.64\\n0.84\\n0.77\\n0.75\\n0.', 'node_id__101': '3a4f79c4-ff88-41a2-9941-a6ec575e4d93', 'metadata___101': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"74\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"3a4f79c4-ff88-41a2-9941-a6ec575e4d93\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__101': '[-0.0492093600332737,0.002657530130818486,0.008977214805781841,-0.029528671875596046,0.02765767276287079,-0.0049738758243620396,-0.030532479286193848 ... (7846 characters truncated) ... 7,-0.015748830512166023,-0.04249212145805359,0.016864409670233727,-0.05668102577328682,0.04663722589612007,-0.009948094375431538,0.04124179482460022]', 'text__101': '46\\n0.66\\n0.35\\n0.28\\n34B\\n0.27\\n0.24\\n0.33\\n0.56\\n0.41\\n0.36\\n0.26\\n0.32\\n0.36\\n0.53\\n0.33\\n0.07\\n0.26\\n0.30\\n0.45\\n0.56\\n0.26\\n0.35\\n70B\\n0.31\\n0.2 ... (2136 characters truncated) ... e prompt (c) are\\nfree of grammatical, spelling and other writing errors, and (d) do not fall into any of the categories described\\nin Section A.5.2.', 'node_id__102': 'd6cb2270-3f72-43aa-95ee-b1080a9b6902', 'metadata___102': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"74\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"d6cb2270-3f72-43aa-95ee-b1080a9b6902\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__102': '[-0.03945086896419525,0.02313345856964588,-0.0038420879282057285,-0.02819398045539856,0.019531410187482834,0.03562150523066521,0.007483062334358692,0 ... (7796 characters truncated) ... .00023599402629770339,0.0041661751456558704,-0.023356640711426735,-0.010518582537770271,0.02797902189195156,0.07357597351074219,0.038634203374385834]', 'text__102': 'Promote or enable criminal activities.\\n2. Promote or enable dangerous behaviors to the user or other people.\\n3. Contain, promote or enable offensiv ... (2010 characters truncated) ... To pass the test, annotators\\nneeded to agree with our criteria on 80% of the answers, and pass the written examples with a score\\nof 4 out of 5.\\n74', 'node_id__103': 'f5233073-70dd-4f0b-9fda-bf6483028f83', 'metadata___103': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"75\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"f5233073-70dd-4f0b-9fda-bf6483028f83\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__103': '[-0.04160007834434509,0.0042624580673873425,-0.021666696295142174,-0.006299999542534351,0.05985827371478081,0.006744795944541693,0.012321275658905506 ... (7824 characters truncated) ... 56845,-0.03271615505218506,-0.014986553229391575,0.0108970757573843,-0.0343596525490284,0.01650281809270382,0.02128870226442814,0.013972227461636066]', 'text__103': '• The third test consisted in measuring the alignment with our quality assessment criteria. The test\\nconsisted of 31 different questions asking the  ... (4168 characters truncated) ... amples to have an overall better\\naverage score than their complement. It is insufficient evidence for contamination if only one of these were\\ntrue.', 'node_id__104': '35c5d2b4-572b-4d41-a75f-3b52eef387b6', 'metadata___104': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"75\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"35c5d2b4-572b-4d41-a75f-3b52eef387b6\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__104': '[-0.014338103123009205,-0.006474675145000219,-3.892310633091256e-05,-0.002099551958963275,0.04757043346762657,-0.0032614206429570913,0.02705727517604 ... (7824 characters truncated) ... 09751430712640285,-0.005034156609326601,0.006561452057212591,-0.025418641045689583,-0.012639285996556282,-0.005089836660772562,-0.009446755051612854]', 'text__104': '(2022), modified to work on a PySpark cluster (effectively without random access to disk). Given the\\nembarrassingly parallel nature of the task, we  ... (1246 characters truncated) ... tly. With the given definition of\\ncontamination (as well as other definitions mentioned in the literature), there is a possibility that a sample\\n75', 'node_id__105': '4816c102-22bb-4d7f-9237-b223573d4e54', 'metadata___105': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"76\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"4816c102-22bb-4d7f-9237-b223573d4e54\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__105': '[-0.024945102632045746,0.00010394136188551784,-0.002446508500725031,-0.00953178945928812,0.07095226645469666,-0.016237301751971245,-0.006012015976011 ... (7857 characters truncated) ... 798998801968992,0.0008685553329996765,0.015512287616729736,-0.021013597026467323,-0.034423913806676865,-0.00042012488120235503,-0.009545068256556988]', 'text__105': 'Dataset\\nModel\\nSubset Type\\nAvg. Contam. %\\nn\\n¯X\\nµn\\nZn\\nHellaSwag (L = 40)\\n70B\\nClean\\n0\\n7391\\n80.0\\n82.5\\n-5.73\\nNot Clean\\n67.5\\n2651\\n89.5\\n ... (2878 characters truncated) ... r dataset (for any\\nchoice of L) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76', 'node_id__106': '9000b2c9-0220-4e1c-b703-1d0e9462415b', 'metadata___106': '{\"total_pages\": 77, \"file_path\": \"./data/llama2.pdf\", \"source\": \"77\", \"_node_content\": \"{\\\\\"id_\\\\\": \\\\\"9000b2c9-0220-4e1c-b703-1d0e9462415b\\\\\", \\\\\"em ... (498 characters truncated) ... }\\\\\\\\n\\\\\\\\n{content}\\\\\", \\\\\"class_name\\\\\": \\\\\"TextNode\\\\\"}\", \"_node_type\": \"TextNode\", \"document_id\": \"None\", \"doc_id\": \"None\", \"ref_doc_id\": \"None\"}', 'embedding__106': '[-0.047984685748815536,-0.011453934945166111,-0.007028207182884216,0.0034323756117373705,0.02276965230703354,0.018554989248514175,-0.0140381306409835 ... (7824 characters truncated) ... ,-0.009772544726729393,-0.03153320774435997,0.02807709388434887,-0.007342176977545023,0.0007125982665456831,0.02753782831132412,0.044340990483760834]', 'text__106': 'A.7\\nModel Card\\nTable 52 presents a model card (Mitchell et al., 2018; Anil et al., 2023) that summarizes details of the models.\\nModel Details\\nMod ... (3240 characters truncated) ...  Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card for Llama 2.\\n77'}]\n(Background on this error at: https://sqlalche.me/e/20/f405)"
     ]
    }
   ],
   "source": [
    "#Store Nodes in the Vector Store\n",
    "vector_store.add(nodes)\n",
    "print(\"Nodes have been added to the vector store!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
